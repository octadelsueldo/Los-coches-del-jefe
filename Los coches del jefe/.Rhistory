skim(data)
summarise_all(data, funs(sum(is.na(.)))) #cuentame los valores nulos en el dataset
#Reemplazamos los valores NA de las observaciones faltantes por la media de su marca
data[116, 11] <- mean(data[c(119, 120, 118, 121, 117), 11])
data[116, 11]
#Reemplazo los valores de la columna 11 entre la observacion 75 a 79 por la media de las 10 observaciones anteriores
data[c(75:79), 11] <- mean(data[c(63:74), 11])
data[19, 11] <- mean(data[c(13:18, 20:22), 11])
data[19, 12] <- mean(data[c(13:18, 20:22), 12])
data[c(105, 106), 12] <- 144
data[114, 12] <- 135
data[7:8,8] <- mean(data[c(9:12),8])
data[26, 11] <- 12
data[26, 14] <- 19
data[61:62, 14] <- mean(data[c(48:60),14])
data[75:79, 10] <- mean(data[c(63:74, 80:81), 10])
data[75:79, 12] <- mean(data[c(63:74, 80:81), 12])
data[75:81, 14] <- mean(data[c(63:74), 14])
data[91, 10] <- mean(data[c(92:94), 10])
data[91, 11] <- mean(data[c(92:94), 11])
data[91:92, 14] <- 22
data[105:106, 13] <- mean(data[c(95:104, 107:113), 13])
data[114, 13] <- 135
data[116, 12] <- mean(data[c(117:121), 12])
#al resto de valores nulos le aplicaremos la media global
data <- na_mean(data)
summarise_all(data, funs(sum(is.na(.)))) #chequeamos los resultados
perfomScaling <-  T
if(perfomScaling){
for(i in names(data)){
if(class(data[,i ]) == 'integer' | class(data[,i ]) == 'numeric'){
data[,i ] = scale(data[,i ])
}
}
}
acp= PCA(data[,c(3, 5:8, 10:14)], graph=T)
fviz_eig(acp, addlabels=TRUE, hjust = -0.3)+
labs(title="Scree plot / Grafico de sedimentacion", x="Dimensiones", y="% Varianza explicada")
theme_minimal()
var=get_pca_var(acp) #factoextra
var$cos2
corrplot(var$cos2, is.corr = FALSE)
## ambos ejes
fviz_contrib(acp, choice="var", axes = 1:3)+
labs(title = "Contribuciones a las tres dimensiones")
data %>%
group_by(marca) %>%
dplyr::summarize(potencia = mean(potencia),
cc = mean(cc),
consumo_urbano = mean(consurb),
velocida = mean(velocida),
rpm = mean(rpm),
peso = mean(peso))
ggplot(data = data, mapping = aes(x = potencia, y = velocida)) +
geom_point(mapping = aes(color = marca)) +
geom_smooth()
ggplot(data = data, mapping = aes(x = rpm, y = velocida)) +
geom_point(mapping = aes(color = marca)) +
geom_smooth()
variables <- c('potencia','cc','rpm','peso','consurb','velocida')
cochesescalados <- subset(data, select = variables)
skim(cochesescalados)
#observamos las diferentes distancias entras las observaciones. Colocamos stand F (FALSE) porq ya estan los valores estandarizados
qdist <- get_dist(cochesescalados, stand = F, method = 'pearson')
str(qdist)
#Visualizamos ahora la matriz de distancias
fviz_dist(qdist, lab_size = 5)
as.matrix(as.dist(qdist))
fviz_dist(qdist, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"), lab_size = 5)
cochescorr <- cor(cochesescalados, method = 'pearson')
round(cochescorr,3)
dist.cor <- as.dist(1 - cochescorr)  # Las correlaciones negativas tendrán en la nueva matriz de distancias un valor > 1
round(as.matrix(dist.cor),  2)
corrplot(as.matrix(qdist), is.corr = FALSE, method = "color", type="lower", diag=F, order="hclust", tl.cex=0.6, tl.col="blue")
coches.eclust <- eclust(cochesescalados, FUNcluster = 'kmeans', stand = F, hc_metric = 'euclidean',  nstart = 25)
coches.eclust.j = eclust(cochesescalados[,-1], "hclust", k = 6)
fviz_cluster(coches.eclust.j)
fviz_silhouette(coches.eclust.j)
fviz_dend(hclust(dist(cochesescalados)), k = 6, cex = 0.5, main = "Dendrograma")
var=get_pca_var(acp) #factoextra
var$contrib
corrplot(var$contrib, is.corr = FALSE)
var=get_pca_var(acp) #factoextra
var$cos2
corrplot(var$cos2, is.corr = FALSE)
## tres ejes
fviz_contrib(acp, choice="var", axes = 1:2)+
labs(title = "Contribuciones a las tres dimensiones")
## tres ejes
fviz_contrib(acp, choice="var", axes = 1:3)+
labs(title = "Contribuciones a las tres dimensiones")
rownames(data) = make.names(substring(data$modelo,1,15), unique = TRUE)
View(data)
data$marca <- factor(data$marca, levels = c('1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17'),
labels = c('ASIA MOTORS','CHEVROLET','DAIHATSU','FORD','JEEP','KIA','LADA','LAND ROVER','MERCEDES'
,'MITSUBISHI','NISSAN','OPEL','SSANGYONG','SUZUKI','TATA','TOYOTA','UAZ'))
library(foreign) #Mediante foreign se dispone del método read.spss para la carga de archivos en R.
library(janitor) # Limpieza de nombres
library(skimr) # Summary lindo
library(magrittr) #  %<>%
library(corrplot) # Grafico de correlaciones
library(ggcorrplot)  # Correlaciones con ggplot
library(PerformanceAnalytics) # Otra correlación
library(imputeTS) # na_mean() reemplaza nulos por la media
library(ggplot2)
library(dplyr)
library(Rtsne)
library(haven)
library(foreign)
library(factoextra)
library(cluster)
library(FactoMineR)
library(imputeTS)
data <- read.spss("tterreno.sav", to.data.frame = TRUE)
head(data)
tail(data)
#Observamos los objetos dentro del dataset
str(data)
skim(data)
#Reemplazamos los valores NA de las observaciones faltantes por la media de su marca
data[116, 11] <- mean(data[c(119, 120, 118, 121, 117), 11])
data[116, 11]
#Reemplazo los valores de la columna 11 entre la observacion 75 a 79 por la media de las 10 observaciones anteriores
data[c(75:79), 11] <- mean(data[c(63:74), 11])
data[19, 11] <- mean(data[c(13:18, 20:22), 11])
data[19, 12] <- mean(data[c(13:18, 20:22), 12])
data[c(105, 106), 12] <- 144
data[114, 12] <- 135
data[7:8,8] <- mean(data[c(9:12),8])
data[26, 11] <- 12
data[26, 14] <- 19
data[61:62, 14] <- mean(data[c(48:60),14])
data[75:79, 10] <- mean(data[c(63:74, 80:81), 10])
data[75:79, 12] <- mean(data[c(63:74, 80:81), 12])
data[75:81, 14] <- mean(data[c(63:74), 14])
data[91, 10] <- mean(data[c(92:94), 10])
data[91, 11] <- mean(data[c(92:94), 11])
data[91:92, 14] <- 22
data[105:106, 13] <- mean(data[c(95:104, 107:113), 13])
data[114, 13] <- 135
data[116, 12] <- mean(data[c(117:121), 12])
#al resto de valores nulos le aplicaremos la media global
data <- na_mean(data)
summarise_all(data, funs(sum(is.na(.)))) #chequeamos los resultados
rownames(data) = make.names(substring(data$modelo,1,15), unique = TRUE)
data$marca <- factor(data$marca, levels =
c('1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17'),
labels = c('ASIA MOTORS','CHEVROLET','DAIHATSU','FORD','JEEP','KIA','LADA','LAND ROVER','MERCEDES','MITSUBISHI','NISSAN','OPEL','SSANGYONG','SUZUKI','TATA','TOYOTA','UAZ'))
data$cilindro <- as.numeric(data$cilindro)
data$plazas <- as.numeric(data$plazas)
data$acel2 <- factor(data$acel2, levels = c('1','2'),
labels = c('Menor a 10 segundos','Mayor a 10 segundos'))
str(data)
knitr::opts_chunk$set(echo = TRUE)
library(foreign) #Mediante foreign se dispone del método read.spss para la carga de archivos en R.
library(janitor) # Limpieza de nombres
library(skimr) # Summary lindo
library(magrittr) #  %<>%
library(corrplot) # Grafico de correlaciones
library(ggcorrplot)  # Correlaciones con ggplot
library(PerformanceAnalytics) # Otra correlación
library(imputeTS) # na_mean() reemplaza nulos por la media
library(ggplot2)
library(dplyr)
library(Rtsne)
library(haven)
library(foreign)
library(factoextra)
library(cluster)
library(FactoMineR)
library(imputeTS)
data <- read.spss("tterreno.sav", to.data.frame = TRUE)
head(data)
tail(data)
#Observamos los objetos dentro del dataset
str(data)
skim(data)
summarise_all(data, funs(sum(is.na(.)))) #cuentame los valores nulos en el dataset
#Reemplazamos los valores NA de las observaciones faltantes por la media de su marca
data[116, 11] <- mean(data[c(119, 120, 118, 121, 117), 11])
data[116, 11]
#Reemplazo los valores de la columna 11 entre la observacion 75 a 79 por la media de las 10 observaciones anteriores
data[c(75:79), 11] <- mean(data[c(63:74), 11])
data[19, 11] <- mean(data[c(13:18, 20:22), 11])
data[19, 12] <- mean(data[c(13:18, 20:22), 12])
data[c(105, 106), 12] <- 144
data[114, 12] <- 135
data[7:8,8] <- mean(data[c(9:12),8])
data[26, 11] <- 12
data[26, 14] <- 19
data[61:62, 14] <- mean(data[c(48:60),14])
data[75:79, 10] <- mean(data[c(63:74, 80:81), 10])
data[75:79, 12] <- mean(data[c(63:74, 80:81), 12])
data[75:81, 14] <- mean(data[c(63:74), 14])
data[91, 10] <- mean(data[c(92:94), 10])
data[91, 11] <- mean(data[c(92:94), 11])
data[91:92, 14] <- 22
data[105:106, 13] <- mean(data[c(95:104, 107:113), 13])
data[114, 13] <- 135
data[116, 12] <- mean(data[c(117:121), 12])
#al resto de valores nulos le aplicaremos la media global
data <- na_mean(data)
summarise_all(data, funs(sum(is.na(.)))) #chequeamos los resultados
perfomScaling <-  T
if(perfomScaling){
for(i in names(data)){
if(class(data[,i ]) == 'integer' | class(data[,i ]) == 'numeric'){
data[,i ] = scale(data[,i ])
}
}
}
acp= PCA(data[,c(3, 5:8, 10:14)], graph=T)
fviz_eig(acp, addlabels=TRUE, hjust = -0.3)+
labs(title="Scree plot / Grafico de sedimentacion", x="Dimensiones", y="% Varianza explicada")
theme_minimal()
var=get_pca_var(acp) #factoextra
var$cos2
corrplot(var$cos2, is.corr = FALSE)
## tres ejes
fviz_contrib(acp, choice="var", axes = 1:3)+
labs(title = "Contribuciones a las tres dimensiones")
data %>%
group_by(marca) %>%
dplyr::summarize(potencia = mean(potencia),
cc = mean(cc),
consumo_urbano = mean(consurb),
velocida = mean(velocida),
rpm = mean(rpm),
peso = mean(peso))
ggplot(data = data, mapping = aes(x = potencia, y = velocida)) +
geom_point(mapping = aes(color = marca)) +
geom_smooth()
ggplot(data = data, mapping = aes(x = rpm, y = velocida)) +
geom_point(mapping = aes(color = marca)) +
geom_smooth()
variables <- c('potencia','cc','rpm','peso','consurb','velocida')
cochesescalados <- subset(data, select = variables)
skim(cochesescalados)
#observamos las diferentes distancias entras las observaciones. Colocamos stand F (FALSE) porq ya estan los valores estandarizados
qdist <- get_dist(cochesescalados, stand = F, method = 'pearson')
str(qdist)
#Visualizamos ahora la matriz de distancias
fviz_dist(qdist, lab_size = 5)
as.matrix(as.dist(qdist))
fviz_dist(qdist, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"), lab_size = 5)
cochescorr <- cor(cochesescalados, method = 'pearson')
round(cochescorr,3)
dist.cor <- as.dist(1 - cochescorr)  # Las correlaciones negativas tendrán en la nueva matriz de distancias un valor > 1
round(as.matrix(dist.cor),  2)
corrplot(as.matrix(qdist), is.corr = FALSE, method = "color", type="lower", diag=F, order="hclust", tl.cex=0.6, tl.col="blue")
coches.eclust <- eclust(cochesescalados, FUNcluster = 'kmeans', stand = F, hc_metric = 'euclidean',  nstart = 25)
coches.eclust.j = eclust(cochesescalados[,-1], "hclust", k = 6)
fviz_cluster(coches.eclust.j)
fviz_silhouette(coches.eclust.j)
fviz_dend(hclust(dist(cochesescalados)), k = 6, cex = 0.5, main = "Dendrograma")
medias_datos <- data %>%
group_by(marca) %>%
dplyr::summarize(potencia = mean(potencia),
cc = mean(cc),
consumo_urbano = mean(consurb),
velocida = mean(velocida),
rpm = mean(rpm),
peso = mean(peso))
plot(medias_datos)
medias_datos <- data %>%
group_by(marca) %>%
dplyr::summarize(potencia = mean(potencia),
cc = mean(cc),
consumo_urbano = mean(consurb),
velocida = mean(velocida),
rpm = mean(rpm),
peso = mean(peso))
medias_datos
ggplot(data = data, mapping = aes(x = potencia, y = velocida)) +
geom_point(mapping = aes(color = marca)) +
geom_smooth()
ggplot(tterreno) + geom_bar(aes(x = cilindro)) + xlab('Nº de cilindros') + ylab('Nº de vehículos')
ggplot(data) + geom_bar(aes(x = cilindro)) + xlab('Nº de cilindros') + ylab('Nº de vehículos')
ggplot(data) + geom_bar(aes(x = plazas)) + xlab('Nº de plazas') + ylab('Nº de vehículos')
ggplot(data, aes(x = velocida, y = acelerac)) + geom_point() + xlab('velocidad') + ylab('aceleración')
ggplot(data, aes(x = peso, y = cons90, color = plazas)) + geom_point() + xlab('Peso (Kg)') + ylab('Consumo a 90 Km/h (Litros)')
ggplot(data, aes(x = peso, y = cons120, color = plazas)) + geom_point() + xlab('Peso (Kg)') + ylab('Consumo a 120 Km/h (Litros)')
ggplot(data, aes(x = peso, y = consurb, color = plazas)) + geom_point() + xlab('Peso (Kg)') + ylab('Consumo urbano (Litros)')
ggplot(data, aes(x = plazas, y = peso)) + geom_boxplot(color = 'red', fill = 'red', alpha = 0.2,
notch = F, notchwidth = 2,
outlier.colour = 'blue', outlier.fill = 'blue', outlier.size = 2) +
stat_summary(fun.y = mean, geom = "point", shape = 20, size = 3, color = "green", fill = "green") +
xlab('Nº de plazas') + ylab('Peso (Kg)')
knitr::opts_chunk$set(echo = TRUE)
library(foreign) #Mediante foreign se dispone del método read.spss para la carga de archivos en R.
library(janitor) # Limpieza de nombres
library(skimr) # Summary lindo
library(magrittr) #  %<>%
library(corrplot) # Grafico de correlaciones
library(ggcorrplot)  # Correlaciones con ggplot
library(PerformanceAnalytics) # Otra correlación
library(imputeTS) # na_mean() reemplaza nulos por la media
library(ggplot2)
library(dplyr)
library(Rtsne)
library(haven)
library(foreign)
library(factoextra)
library(cluster)
library(FactoMineR)
library(imputeTS)
data <- read.spss("data.sav", to.data.frame = TRUE)
library(foreign) #Mediante foreign se dispone del método read.spss para la carga de archivos en R.
library(janitor) # Limpieza de nombres
library(skimr) # Summary lindo
library(magrittr) #  %<>%
library(corrplot) # Grafico de correlaciones
library(ggcorrplot)  # Correlaciones con ggplot
library(PerformanceAnalytics) # Otra correlación
library(imputeTS) # na_mean() reemplaza nulos por la media
library(ggplot2)
library(dplyr)
library(Rtsne)
library(haven)
library(foreign)
library(factoextra)
library(cluster)
library(FactoMineR)
library(imputeTS)
data <- read.spss("data.sav", to.data.frame = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(foreign) #Mediante foreign se dispone del método read.spss para la carga de archivos en R.
library(janitor) # Limpieza de nombres
library(skimr) # Summary lindo
library(magrittr) #  %<>%
library(corrplot) # Grafico de correlaciones
library(ggcorrplot)  # Correlaciones con ggplot
library(PerformanceAnalytics) # Otra correlación
library(imputeTS) # na_mean() reemplaza nulos por la media
library(ggplot2)
library(dplyr)
library(Rtsne)
library(haven)
library(foreign)
library(factoextra)
library(cluster)
library(FactoMineR)
library(imputeTS)
data <- read.spss("tterreno.sav", to.data.frame = TRUE)
head(data)
tail(data)
#Observamos los objetos dentro del dataset
str(data)
skim(data)
summarise_all(data, funs(sum(is.na(.)))) #cuentame los valores nulos en el dataset
#Reemplazamos los valores NA de las observaciones faltantes por la media de su marca
data[116, 11] <- mean(data[c(119, 120, 118, 121, 117), 11])
data[116, 11]
data[c(75:79), 11] <- mean(data[c(63:74), 11])
data[19, 11] <- mean(data[c(13:18, 20:22), 11])
data[19, 12] <- mean(data[c(13:18, 20:22), 12])
data[c(105, 106), 12] <- 144
data[114, 12] <- 135
data[7:8,8] <- mean(data[c(9:12),8])
data[26, 11] <- 12
data[26, 14] <- 19
data[61:62, 14] <- mean(data[c(48:60),14])
data[75:79, 10] <- mean(data[c(63:74, 80:81), 10])
data[75:79, 12] <- mean(data[c(63:74, 80:81), 12])
data[75:81, 14] <- mean(data[c(63:74), 14])
data[91, 10] <- mean(data[c(92:94), 10])
data[91, 11] <- mean(data[c(92:94), 11])
data[91:92, 14] <- 22
data[105:106, 13] <- mean(data[c(95:104, 107:113), 13])
data[114, 13] <- 135
data[116, 12] <- mean(data[c(117:121), 12])
#al resto de valores nulos le aplicaremos la media global
data <- na_mean(data)
summarise_all(data, funs(sum(is.na(.)))) #chequeamos los resultados
perfomScaling <-  T
if(perfomScaling){
for(i in names(data)){
if(class(data[,i ]) == 'integer' | class(data[,i ]) == 'numeric'){
data[,i ] = scale(data[,i ])
}
}
}
ggplot(data) + geom_bar(aes(x = cilindro)) + xlab('Nº de cilindros') + ylab('Nº de vehículos')
ggplot(data) + geom_bar(aes(x = plazas)) + xlab('Nº de plazas') + ylab('Nº de vehículos')
ggplot(data, aes(x = velocida, y = acelerac)) + geom_point() + xlab('velocidad') + ylab('aceleración')
ggplot(data, aes(x = peso, y = cons90, color = plazas)) + geom_point() + xlab('Peso (Kg)') + ylab('Consumo a 90 Km/h (Litros)')
ggplot(data, aes(x = peso, y = cons120, color = plazas)) + geom_point() + xlab('Peso (Kg)') + ylab('Consumo a 120 Km/h (Litros)')
ggplot(data, aes(x = peso, y = consurb, color = plazas)) + geom_point() + xlab('Peso (Kg)') + ylab('Consumo urbano (Litros)')
ggplot(data, aes(x = plazas, y = peso)) + geom_boxplot(color = 'red', fill = 'red', alpha = 0.2,
notch = F, notchwidth = 2,
outlier.colour = 'blue', outlier.fill = 'blue', outlier.size = 2) +
stat_summary(fun.y = mean, geom = "point", shape = 20, size = 3, color = "green", fill = "green") +
xlab('Nº de plazas') + ylab('Peso (Kg)')
acp= PCA(data[,c(3, 5:8, 10:14)], graph=T)
fviz_eig(acp, addlabels=TRUE, hjust = -0.3)+
labs(title="Scree plot / Grafico de sedimentacion", x="Dimensiones", y="% Varianza explicada")
theme_minimal()
var=get_pca_var(acp) #factoextra
var$cos2
corrplot(var$cos2, is.corr = FALSE)
## tres ejes
fviz_contrib(acp, choice="var", axes = 1:3)+
labs(title = "Contribuciones a las tres dimensiones")
medias_datos <- data %>%
group_by(marca) %>%
dplyr::summarize(potencia = mean(potencia),
cc = mean(cc),
consumo_urbano = mean(consurb),
velocida = mean(velocida),
rpm = mean(rpm),
peso = mean(peso))
medias_datos
ggplot(data = data, mapping = aes(x = potencia, y = velocida)) +
geom_point(mapping = aes(color = marca)) +
geom_smooth()
ggplot(data = data, mapping = aes(x = rpm, y = velocida)) +
geom_point(mapping = aes(color = marca)) +
geom_smooth()
variables <- c('potencia','cc','rpm','peso','consurb','velocida')
cochesescalados <- subset(data, select = variables)
skim(cochesescalados)
#observamos las diferentes distancias entras las observaciones. Colocamos stand F (FALSE) porq ya estan los valores estandarizados
qdist <- get_dist(cochesescalados, stand = F, method = 'pearson')
str(qdist)
#Visualizamos ahora la matriz de distancias
fviz_dist(qdist, lab_size = 5)
as.matrix(as.dist(qdist))
fviz_dist(qdist, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"), lab_size = 5)
cochescorr <- cor(cochesescalados, method = 'pearson')
round(cochescorr,3)
dist.cor <- as.dist(1 - cochescorr)  # Las correlaciones negativas tendrán en la nueva matriz de distancias un valor > 1
round(as.matrix(dist.cor),  2)
corrplot(as.matrix(qdist), is.corr = FALSE, method = "color", type="lower", diag=F, order="hclust", tl.cex=0.6, tl.col="blue")
coches.eclust <- eclust(cochesescalados, FUNcluster = 'kmeans', stand = F, hc_metric = 'euclidean',  nstart = 25)
coches.eclust.j = eclust(cochesescalados[,-1], "hclust", k = 6)
fviz_cluster(coches.eclust.j)
fviz_silhouette(coches.eclust.j)
fviz_dend(hclust(dist(cochesescalados)), k = 6, cex = 0.5, main = "Dendrograma")
motor_numeric <- data[, c(5,  6, 7, 10:14)]
str(motor_numeric)
corr_motor <- cor(motor_numeric, use = "complete.obs")
corrplot(corr_motor)
ggplot(data, aes(x = plazas, y = peso)) +
geom_boxplot(color = 'blue',
fill = 'blue',
alpha = 0.2,
notch = F,
notchwidth = 2,
outlier.colour = 'red',
outlier.fill = 'red',
outlier.size = 2) +
stat_summary(fun.y = mean,
geom = "point",
shape = 20,
size = 3,
color = "yellow",
fill = "yellow") +
xlab('Nº de plazas') +
ylab('Peso (Kg)')
ggplot(data, aes(x = plazas, y = peso)) +
geom_boxplot(color = 'blue',
fill = 'blue',
alpha = 0.2,
notch = F,
notchwidth = 2,
outlier.colour = 'red',
outlier.size = 2) +
stat_summary(fun.y = mean,
geom = "point",
shape = 20,
size = 3,
color = "yellow",
fill = "yellow") +
xlab('Nº de plazas') +
ylab('Peso (Kg)')
ggplot(data, aes(x = plazas, y = peso)) +
geom_boxplot(color = 'blue',
fill = 'blue',
alpha = 0.2,
notch = F,
notchwidth = 2,
outlier.colour = 'black',
outlier.size = 2) +
stat_summary(fun.y = mean,
geom = "point",
shape = 20,
size = 3,
color = "yellow",
fill = "yellow") +
xlab('Nº de plazas') +
ylab('Peso (Kg)')
ggplot(data, aes(x = plazas, y = peso)) +
geom_boxplot(color = 'blue',
fill = 'blue',
alpha = 0.2,
notch = F,
notchwidth = 2,
outlier.colour = 'purple',
outlier.size = 2) +
stat_summary(fun.y = mean,
geom = "point",
shape = 20,
size = 3,
color = "yellow",
fill = "yellow") +
xlab('Nº de plazas') +
ylab('Peso (Kg)')
knitr::opts_chunk$set(echo = TRUE, fig.align = 'center')
ggplot(data, aes(x = velocida, y = acelerac, color = marca)) +
geom_point() +
xlab('velocidad') +
ylab('aceleración')
