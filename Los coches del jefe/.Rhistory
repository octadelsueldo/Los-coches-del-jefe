coches.eclust.agnes = eclust(data_num, "agnes", hc_metric = 'euclidean', hc_method = 'ward.D2')
fviz_cluster(coches.eclust.agnes)
#Silueta con el AGNES
fviz_silhouette(coches.eclust.agnes)
coches.eclust.agnes$cluster
coches.eclust.agnes$silinfo
# dendrograma con 10 grupos
fviz_dend(coches.eclust.agnes, rect = TRUE)
fviz_dend(coches.eclust.hc, k = 10, cex = 0.5, main = "Dendrograma")
coches.eclust.hc = eclust(data_num, "hclust", hc_metric = 'euclidean',hc_method = 'ward.D2')
fviz_cluster(coches.eclust.j)
fviz_silhouette(coches.eclust.hc)
fviz_dend(coches.eclust.hc, k = 10, cex = 0.5, main = "Dendrograma")
#hclust(dist(data_num))
#Metodo Agnes
coches.eclust.agnes = eclust(data_num, "agnes", hc_metric = 'euclidean', hc_method = 'ward.D2')
fviz_cluster(coches.eclust.agnes)
#Silueta con el AGNES
fviz_silhouette(coches.eclust.agnes)
# dendrograma con 10 grupos
fviz_dend(coches.eclust.agnes, rect = TRUE)
#Dividimos en tres grupos a la vista del grafo anterior:
grp = cutree(coches.eclust.hc, k = 10)
#Y observamos el número de elementos de cada segmento:
table(grp)
knitr::opts_chunk$set(echo = F, warning=F, fig.align = "center", out.width='50%', out.height='50%')
library(foreign) #Mediante foreign se dispone del método read.spss para la carga de archivos en R.
library(janitor) # Limpieza de nombres
library(skimr) # Summary lindo
library(magrittr) #  %<>%
library(corrplot) # Grafico de correlaciones
library(ggcorrplot)  # Correlaciones con ggplot
library(PerformanceAnalytics) # Otra correlación
library(imputeTS) # na_mean() reemplaza nulos por la media
library(ggplot2)
library(dplyr)
library(Rtsne)
library(haven)
library(foreign)
library(factoextra)
library(cluster)
library(FactoMineR)
library(imputeTS)
data <- read.spss("tterreno.sav", to.data.frame = TRUE)
head(data)
tail(data)
#Observamos los objetos dentro del dataset
str(data)
skim(data)
summarise_all(data, funs(sum(is.na(.)))) #cuentame los valores nulos en el dataset
#Reemplazamos los valores NA de las observaciones faltantes por la media de su marca para no perder informacion
data[116, 11] <- mean(data[c(119, 120, 118, 121, 117), 11])
data[c(75:79), 11] <- mean(data[c(63:74), 11])
data[19, 11] <- mean(data[c(13:18, 20:22), 11])
data[19, 12] <- mean(data[c(13:18, 20:22), 12])
data[c(105, 106), 12] <- 144
data[114, 12] <- 135
data[7:8,8] <- mean(data[c(9:12),8])
data[26, 11] <- 12
data[26, 14] <- 19
data[61:62, 14] <- mean(data[c(48:60),14])
data[75:79, 10] <- mean(data[c(63:74, 80:81), 10])
data[75:79, 12] <- mean(data[c(63:74, 80:81), 12])
data[75:81, 14] <- mean(data[c(63:74), 14])
data[91, 10] <- mean(data[c(92:94), 10])
data[91, 11] <- mean(data[c(92:94), 11])
data[91:92, 14] <- 22
data[105:106, 13] <- mean(data[c(95:104, 107:113), 13])
data[114, 13] <- 135
data[116, 12] <- mean(data[c(117:121), 12])
#al resto de valores nulos le aplicaremos la media global ya que no se cuenta con informacion sobre ellos
data <- na_mean(data)
summarise_all(data, funs(sum(is.na(.)))) #chequeamos los resultados
data$cilindro <- as.numeric(data$cilindro) # pasamos a numerica cilindro
data$plazas <- as.numeric(data$plazas) # pasamos a numerica a plazas
str(data) # chequeamos los resultados
# Le colocamos de indice el nombre de los modelos
rownames(data) = make.names(substring(data$modelo,1,15), unique = TRUE)
data_num <- data[,-c(1,2,15)] #elegimos las variables numericas
rownames(data_num) <- rownames(data) #a data_num le ponemos los nombres de data
perfomScaling <-  T
if(perfomScaling){
for(i in names(data_num)){
if(class(data_num[,i ]) == 'integer' | class(data_num[,i ]) == 'numeric'){
data_num[,i ] = scale(data_num[,i ])
}
}
}
skim(data_num)
#observamos las diferentes distancias entras las observaciones. Colocamos stand F (FALSE) porq ya estan los valores estandarizados
qdist <- get_dist(data_num, stand = F, method = 'pearson')
str(qdist)
#Visualizamos ahora la matriz de distancias
fviz_dist(qdist, lab_size = 5)
as.matrix(as.dist(qdist))
fviz_dist(qdist, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"), lab_size = 5)
cochescorr <- cor(data_num, method = 'pearson')
round(cochescorr,3)
dist.cor <- as.dist(1 - cochescorr)  # Las correlaciones negativas tendrán en la nueva matriz de distancias un valor > 1
round(as.matrix(dist.cor),  2)
corrplot(as.matrix(qdist), is.corr = FALSE, method = "color", type="lower", diag=F, order="hclust", tl.cex=0.6, tl.col="blue")
coches.eclust.hc = eclust(data_num, "hclust", hc_metric = 'euclidean',hc_method = 'ward.D2')
fviz_cluster(coches.eclust.j)
knitr::opts_chunk$set(echo = F, warning=F, fig.align = "center", out.width='50%', out.height='50%')
library(foreign) #Mediante foreign se dispone del método read.spss para la carga de archivos en R.
library(janitor) # Limpieza de nombres
library(skimr) # Summary lindo
library(magrittr) #  %<>%
library(corrplot) # Grafico de correlaciones
library(ggcorrplot)  # Correlaciones con ggplot
library(PerformanceAnalytics) # Otra correlación
library(imputeTS) # na_mean() reemplaza nulos por la media
library(ggplot2)
library(dplyr)
library(Rtsne)
library(haven)
library(foreign)
library(factoextra)
library(cluster)
library(FactoMineR)
library(imputeTS)
data <- read.spss("tterreno.sav", to.data.frame = TRUE)
head(data)
tail(data)
#Observamos los objetos dentro del dataset
str(data)
skim(data)
summarise_all(data, funs(sum(is.na(.)))) #cuentame los valores nulos en el dataset
#Reemplazamos los valores NA de las observaciones faltantes por la media de su marca para no perder informacion
data[116, 11] <- mean(data[c(119, 120, 118, 121, 117), 11])
data[c(75:79), 11] <- mean(data[c(63:74), 11])
data[19, 11] <- mean(data[c(13:18, 20:22), 11])
data[19, 12] <- mean(data[c(13:18, 20:22), 12])
data[c(105, 106), 12] <- 144
data[114, 12] <- 135
data[7:8,8] <- mean(data[c(9:12),8])
data[26, 11] <- 12
data[26, 14] <- 19
data[61:62, 14] <- mean(data[c(48:60),14])
data[75:79, 10] <- mean(data[c(63:74, 80:81), 10])
data[75:79, 12] <- mean(data[c(63:74, 80:81), 12])
data[75:81, 14] <- mean(data[c(63:74), 14])
data[91, 10] <- mean(data[c(92:94), 10])
data[91, 11] <- mean(data[c(92:94), 11])
data[91:92, 14] <- 22
data[105:106, 13] <- mean(data[c(95:104, 107:113), 13])
data[114, 13] <- 135
data[116, 12] <- mean(data[c(117:121), 12])
#al resto de valores nulos le aplicaremos la media global ya que no se cuenta con informacion sobre ellos
data <- na_mean(data)
summarise_all(data, funs(sum(is.na(.)))) #chequeamos los resultados
data$cilindro <- as.numeric(data$cilindro) # pasamos a numerica cilindro
data$plazas <- as.numeric(data$plazas) # pasamos a numerica a plazas
str(data) # chequeamos los resultados
# Le colocamos de indice el nombre de los modelos
rownames(data) = make.names(substring(data$modelo,1,15), unique = TRUE)
data_num <- data[,-c(1,2,15)] #elegimos las variables numericas
rownames(data_num) <- rownames(data) #a data_num le ponemos los nombres de data
perfomScaling <-  T
if(perfomScaling){
for(i in names(data_num)){
if(class(data_num[,i ]) == 'integer' | class(data_num[,i ]) == 'numeric'){
data_num[,i ] = scale(data_num[,i ])
}
}
}
skim(data_num)
#observamos las diferentes distancias entras las observaciones. Colocamos stand F (FALSE) porq ya estan los valores estandarizados
qdist <- get_dist(data_num, stand = F, method = 'pearson')
str(qdist)
#Visualizamos ahora la matriz de distancias
fviz_dist(qdist, lab_size = 5)
as.matrix(as.dist(qdist))
fviz_dist(qdist, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"), lab_size = 5)
cochescorr <- cor(data_num, method = 'pearson')
round(cochescorr,3)
dist.cor <- as.dist(1 - cochescorr)  # Las correlaciones negativas tendrán en la nueva matriz de distancias un valor > 1
round(as.matrix(dist.cor),  2)
corrplot(as.matrix(qdist), is.corr = FALSE, method = "color", type="lower", diag=F, order="hclust", tl.cex=0.6, tl.col="blue")
coches.eclust.hc = eclust(data_num, "hclust", hc_metric = 'euclidean',hc_method = 'ward.D2')
fviz_cluster(coches.eclust.hc)
fviz_silhouette(coches.eclust.hc)
fviz_dend(coches.eclust.hc, k = 10, cex = 0.5, main = "Dendrograma")
#hclust(dist(data_num))
#Dividimos en tres grupos a la vista del grafo anterior:
grp = cutree(coches.eclust.hc, k = 10)
#Y observamos el número de elementos de cada segmento:
table(grp)
#  identificar quiénes pertenecen a cada grupo
rownames(data_num)[grp==1]
aggregate(data[,3:14],list(grp),median)
#Metodo Agnes
coches.eclust.agnes = eclust(data_num, "agnes", hc_metric = 'euclidean', hc_method = 'ward.D2')
fviz_cluster(coches.eclust.agnes)
#Silueta con el AGNES
fviz_silhouette(coches.eclust.agnes)
# dendrograma con 10 grupos
fviz_dend(coches.eclust.agnes, rect = TRUE)
grp.ag = cutree(as.hclust(coches.eclust.agnes), k = 10)
table(grp.ag)
rownames(data_num)[grp.ag==1]
aggregate(data[,3:14],list(grp.ag),median)
library(dendextend)
#Cálculo del jerárquico por hClust y Agnes
#Hacemos los dendrogramas
coches.eclust.hc = as.dendrogram(coches.eclust.hc)
coches.eclust.agnes = as.dendrogram(coches.eclust.agnes)
#creamos una lista con ellos
dend_list = dendlist(coches.eclust.hc, coches.eclust.agnes)
tanglegram(coches.eclust.hc, coches.eclust.agnes)
tanglegram(coches.eclust.hc, coches.eclust.agnes,
highlight_distinct_edges = FALSE, # Omite las líneas punteadas
common_subtrees_color_lines = FALSE, # Omite las líneas de colores
common_subtrees_color_branches = TRUE, # Colorea las ramas comunes
main = paste("Entanglement =", round(entanglement(dend_list), 2))
)
require(dendextend)
# Matriz de correlación de Baker
cor.dendlist(dend_list, method = "baker")
#  Matriz de correlación Cofenética
cor.dendlist(dend_list, method = "cophenetic")
#Metodo Agnes
coches.eclust.agnes = eclust(data_num, "diana", hc_metric = 'euclidean', hc_method = 'ward.D2')
fviz_cluster(coches.eclust.agnes)
#Silueta con el AGNES
fviz_silhouette(coches.eclust.agnes)
# dendrograma con 10 grupos
fviz_dend(coches.eclust.agnes, rect = TRUE)
knitr::opts_chunk$set(echo = F, warning=F, fig.align = "center", out.width='50%', out.height='50%')
library(foreign) #Mediante foreign se dispone del método read.spss para la carga de archivos en R.
library(janitor) # Limpieza de nombres
library(skimr) # Summary lindo
library(magrittr) #  %<>%
library(corrplot) # Grafico de correlaciones
library(ggcorrplot)  # Correlaciones con ggplot
library(PerformanceAnalytics) # Otra correlación
library(imputeTS) # na_mean() reemplaza nulos por la media
library(ggplot2)
library(dplyr)
library(Rtsne)
library(haven)
library(foreign)
library(factoextra)
library(cluster)
library(FactoMineR)
library(imputeTS)
data <- read.spss("tterreno.sav", to.data.frame = TRUE)
head(data)
tail(data)
#Observamos los objetos dentro del dataset
str(data)
skim(data)
summarise_all(data, funs(sum(is.na(.)))) #cuentame los valores nulos en el dataset
#Reemplazamos los valores NA de las observaciones faltantes por la media de su marca para no perder informacion
data[116, 11] <- mean(data[c(119, 120, 118, 121, 117), 11])
data[c(75:79), 11] <- mean(data[c(63:74), 11])
data[19, 11] <- mean(data[c(13:18, 20:22), 11])
data[19, 12] <- mean(data[c(13:18, 20:22), 12])
data[c(105, 106), 12] <- 144
data[114, 12] <- 135
data[7:8,8] <- mean(data[c(9:12),8])
data[26, 11] <- 12
data[26, 14] <- 19
data[61:62, 14] <- mean(data[c(48:60),14])
data[75:79, 10] <- mean(data[c(63:74, 80:81), 10])
data[75:79, 12] <- mean(data[c(63:74, 80:81), 12])
data[75:81, 14] <- mean(data[c(63:74), 14])
data[91, 10] <- mean(data[c(92:94), 10])
data[91, 11] <- mean(data[c(92:94), 11])
data[91:92, 14] <- 22
data[105:106, 13] <- mean(data[c(95:104, 107:113), 13])
data[114, 13] <- 135
data[116, 12] <- mean(data[c(117:121), 12])
#al resto de valores nulos le aplicaremos la media global ya que no se cuenta con informacion sobre ellos
data <- na_mean(data)
summarise_all(data, funs(sum(is.na(.)))) #chequeamos los resultados
data$cilindro <- as.numeric(data$cilindro) # pasamos a numerica cilindro
data$plazas <- as.numeric(data$plazas) # pasamos a numerica a plazas
str(data) # chequeamos los resultados
# Le colocamos de indice el nombre de los modelos
rownames(data) = make.names(substring(data$modelo,1,15), unique = TRUE)
data_num <- data[,-c(1,2,15)] #elegimos las variables numericas
rownames(data_num) <- rownames(data) #a data_num le ponemos los nombres de data
perfomScaling <-  T
if(perfomScaling){
for(i in names(data_num)){
if(class(data_num[,i ]) == 'integer' | class(data_num[,i ]) == 'numeric'){
data_num[,i ] = scale(data_num[,i ])
}
}
}
skim(data_num)
#observamos las diferentes distancias entras las observaciones. Colocamos stand F (FALSE) porq ya estan los valores estandarizados
qdist <- get_dist(data_num, stand = F, method = 'pearson')
str(qdist)
#Visualizamos ahora la matriz de distancias
fviz_dist(qdist, lab_size = 5)
as.matrix(as.dist(qdist))
fviz_dist(qdist, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"), lab_size = 5)
cochescorr <- cor(data_num, method = 'pearson')
round(cochescorr,3)
dist.cor <- as.dist(1 - cochescorr)  # Las correlaciones negativas tendrán en la nueva matriz de distancias un valor > 1
round(as.matrix(dist.cor),  2)
corrplot(as.matrix(qdist), is.corr = FALSE, method = "color", type="lower", diag=F, order="hclust", tl.cex=0.6, tl.col="blue")
coches.eclust.hc = eclust(data_num, "hclust", hc_metric = 'euclidean',hc_method = 'ward.D2')
fviz_cluster(coches.eclust.hc)
fviz_silhouette(coches.eclust.hc)
fviz_dend(coches.eclust.hc, k = 10, cex = 0.5, main = "Dendrograma")
#hclust(dist(data_num))
#Dividimos en tres grupos a la vista del grafo anterior:
grp = cutree(coches.eclust.hc, k = 10)
#Y observamos el número de elementos de cada segmento:
table(grp)
#  identificar quiénes pertenecen a cada grupo
rownames(data_num)[grp==1]
aggregate(data[,3:14],list(grp),median)
#Metodo Agnes
coches.eclust.agnes = eclust(data_num, "agnes", hc_metric = 'euclidean', hc_method = 'ward.D2')
fviz_cluster(coches.eclust.agnes)
#Silueta con el AGNES
fviz_silhouette(coches.eclust.agnes)
# dendrograma con 10 grupos
fviz_dend(coches.eclust.agnes, rect = TRUE)
grp.ag = cutree(as.hclust(coches.eclust.agnes), k = 10)
table(grp.ag)
rownames(data_num)[grp.ag==1]
aggregate(data[,3:14],list(grp.ag),median)
library(dendextend)
#Cálculo del jerárquico por hClust y Agnes
#Hacemos los dendrogramas
coches.eclust.hc = as.dendrogram(coches.eclust.hc)
coches.eclust.agnes = as.dendrogram(coches.eclust.agnes)
#creamos una lista con ellos
dend_list = dendlist(coches.eclust.hc, coches.eclust.agnes)
tanglegram(coches.eclust.hc, coches.eclust.agnes)
tanglegram(coches.eclust.hc, coches.eclust.agnes,
highlight_distinct_edges = FALSE, # Omite las líneas punteadas
common_subtrees_color_lines = FALSE, # Omite las líneas de colores
common_subtrees_color_branches = TRUE, # Colorea las ramas comunes
main = paste("Entanglement =", round(entanglement(dend_list), 2))
)
require(dendextend)
# Matriz de correlación de Baker
cor.dendlist(dend_list, method = "baker")
#  Matriz de correlación Cofenética
cor.dendlist(dend_list, method = "cophenetic")
library(tidyverse)
library(quantmod)
library(forecast)
library(fGarch)
library(patchwork)
#Yahoo ticker (stock or index)
sSymbol="BMW.DE"
#get data from yahoo
mData<-getSymbols(sSymbol ,from="1990-01-01",to="2020-10-31",auto.assign=FALSE)
#Define workdata
xData=Ad(mData)
#Calculate Daily Arithmetic Return
dRentCont=dailyReturn(xData,type='log',leading=FALSE)
#Exclude NA (First data)
dRentCont=na.exclude(dRentCont)
autoplot(xData)
autoplot(dRentCont)
autoplot(dRentCont^2)
# using patchwork
autoplot(xData)/autoplot(dRentCont)/autoplot(dRentCont^2)
# Mean model
#testing mean
t.test(as.vector(dRentCont))
# Mean model
#testing mean
t.test(as.vector(dRentCont))
#ACF y PACF
ggtsdisplay(dRentCont)
#Ljung-Box Test
Box.test(dRentCont,lag=10,  type="Lj")
Box.test(dRentCont,lag=20,  type="Lj")
Box.test(dRentCont,lag=40,  type="Lj")
meanmodel <- auto.arima(xData,lambda=0)
summary(meanmodel)
residuos_arima <- meanmodel$residuals
ggtsdisplay(residuos_arima)
Box.test(residuos_arima,lag=10,  type="Lj")
Box.test(residuos_arima,lag=20,  type="Lj")
Box.test(residuos_arima,lag=40,  type="Lj")
# GARCH?
#ACF y PACF
ggtsdisplay(residuos_arima^2)
#Ljung-Box Test
Box.test(residuos_arima^2,lag=10,  type="Lj")
Box.test(residuos_arima^2,lag=20,  type="Lj")
Box.test(residuos_arima^2,lag=40,  type="Lj")
#GARCH(1,1)
modelo_garch=garchFit(~1+arma(0,4)+garch(1,1),data=dRentCont,trace=F) # Fit an GARCH(1,1) model
summary(modelo_garch)
plot(modelo_garch, which = 2)
plot(modelo_garch, which = 13)
#GARCH(1,1)
modelo_garch_t=garchFit(~1+arma(0,4)+garch(1,1),data=dRentCont,trace=F,cond.dist="std") # Fit an GARCH(1,1) model
summary(modelo_garch_t)
#GARCH(1,1)
modelo_garch_t=garchFit(~1+arma(0,4)+garch(1,1),data=dRentCont,trace=F,cond.dist="std") # Fit an GARCH(1,1) model
summary(modelo_garch_t)
plot(modelo_garch_t, which = 2)
plot(modelo_garch_t, which = 13)
predict(modelo_garch_t,n.ahead=60,plot=TRUE,conf=.9,nx=250) # plot 250 data with 90% confidence
dia="/2020-09-08"
modelo_garch_t=garchFit(~1+arma(0,4)+garch(1,1),data=dRentCont[dia],trace=F,cond.dist="std")
summary(modelo_garch_t)
pred <- predict(modelo_garch_t,n.ahead=1)
sign(pred$meanForecast)
dRentCont["2020-09-09"]
library(tidyverse)
library(quantmod)
library(forecast)
library(vars)
library(patchwork)
#get data from yahoo
BMW=getSymbols("BMW.DE",env=NULL, from="1997-01-01",to="2020-10-31")
VOL=getSymbols("VOW.DE",env=NULL,from="1997-01-01",to="2020-10-31")
# Generar rentabilidad mensual
rBMW=monthlyReturn(BMW[,6])
rVOL=monthlyReturn(VOL[,6])
autoplot(BMW[,6])/autoplot(VOL[,6]) #grafico de las series y sus rentabilidades
autoplot(rBMW)/autoplot(rVOL)
#generar vector
vY=cbind(rBMW["2009-01-01/"],rVOL["2009-01-01/"])
colnames(vY)=c("BMW","VOL")
vY=na.omit(vY)
#Seleccionar modelo
VARselect(vY)
#estimar
model.var=VAR(vY,p=3)
summary(model.var)
#estimar
model.var1=VAR(vY,p=1)
summary(model.var1)
#causalidad de granger
causality(model.var)
causality(model.var1)
#respuesta al impulso
model.ri=irf(model.var)
model.ri
plot(model.ri)
##prediccion
predict(model.var, n.ahead = 8, ci = 0.95)
setwd("~/Documents/CUNEF/Prediccion")
require(devtools)
install_version("PairTrading", version = "1.1", repos = "http://cran.us.r-project.org")
library(PairTrading)
library(PairTrading)
#select 2 stocks
price.pair <- stock.price[,1:2]["2008-12-31::"] #selecciono de 2008 en adelante
#load sample stock price data
data(stock.price) #datos que vienen en la libreria de eke,mplo
adf.test(price.pair[,1],k=0)  #cuando hacemos diky fuller k es igual a 0. Estamos suponiendo que los errores son ruido blanco. Aca vemos que no es estacionaria por el p value
require(devtools)
install_version("PairTrading", version = "1.1", repos = "http://cran.us.r-project.org")
library(PairTrading)
library(PairTrading)
#select 2 stocks
price.pair <- stock.price[,1:2]["2008-12-31::"] #selecciono de 2008 en adelante
adf.test(price.pair[,1],k=0)  #cuando hacemos diky fuller k es igual a 0. Estamos suponiendo que los errores son ruido blanco. Aca vemos que no es estacionaria por el p value
adf.test(price.pair[,2],k=0) #cuando hacemos en el par dos es exactamente igual osea no estacionaria.
#load sample stock price data
data(stock.price) #datos que vienen en la libreria de eke,mplo
library(urca)
test_1<-ur.df(price.pair[,1],type="none",selectlags="AIC",lags=10) #seleciona con aic el k como maximo 10
summary(test_1)
test_2<-ur.df(price.pair[,1],type="trend",selectlags="AIC",lags=10) #para el segundo selecioname la tendencia
summary(test_2)
#Estimate parameters & plot spread
reg <- EstimateParameters(price.pair, method = lm) #estima los parametros de la relacion de equilibro que hay entre los dos pares
str(reg)
plot(reg$spread)
#check stationarity
IsStationary(reg$spread, 0.1) #nos
#estimate parameters for back test
params <- EstimateParametersHistorically(price.pair, period = 180)
#create & plot trading signals
signal <- Simple(params$spread, 0.05) #creo la senal cuando debo entrar y salir en funcion del sperad
barplot(signal,col="blue",space = 0, border = "blue",xaxt="n",yaxt="n",xlab="",ylab="")
par(new=TRUE)
plot(params$spread)
par(new=TRUE)
barplot(signal,col="blue",space = 0, border = "blue",xaxt="n",yaxt="n",xlab="",ylab="")
plot(params$spread)
#Performance of pair trading
return.pairtrading <- Return(price.pair, lag(signal), lag(params$hedge.ratio))
plot(100 * cumprod(1 + return.pairtrading))
par(new=TRUE)
barplot(signal,col="blue",space = 0, border = "blue",xaxt="n",yaxt="n",xlab="",ylab="")
plot(params$spread)
par(new=TRUE)
barplot(signal,col="blue",space = 0, border = "blue",xaxt="n",yaxt="n",xlab="",ylab="")
# load required packages
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(xts)
library(CausalImpact)
install.packages("BoomSpikeSlab")
library(CausalImpact)
library(BoomSpikeSlab)
library(BoomSpikeSlab)
# prep
x <- c("rvest", "dplyr", "lubridate", "stringr", "CausalImpact", "knitr")
purrr::walk(x, library, character.only = TRUE)
library(ggplot2)
library(dplyr)
library(prophet)
